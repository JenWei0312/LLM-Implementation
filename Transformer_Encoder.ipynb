{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "420c8743-10fe-4c5a-9f07-61e1fac2e737",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import math\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Self-Attention Module\n",
    "    Args:\n",
    "        d_model: Total dimension of the model.\n",
    "        d_embed: Embedding dimension.\n",
    "        num_head: Number of attention heads.\n",
    "        dropout: Dropout rate for attention scores.\n",
    "        bias: Whether to include bias in linear projections.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, d_embed, num_head, dropout=0.0, bias=True): # infer d_k, d_v, d_q from d_model\n",
    "        super().__init__()  # Missing in the original implementation\n",
    "        assert d_model % num_head == 0, \"d_model must be divisible by num_head\"\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.d_embed = d_embed\n",
    "        self.num_head = num_head\n",
    "        self.d_head=d_model//num_head\n",
    "        self.dropout_rate = dropout  # Store dropout rate separately\n",
    "\n",
    "        # linear transformations\n",
    "        self.q_proj = nn.Linear(d_embed, d_model, bias=bias)\n",
    "        self.k_proj = nn.Linear(d_embed, d_model, bias=bias)\n",
    "        self.v_proj = nn.Linear(d_embed, d_model, bias=bias)\n",
    "        self.output_proj = nn.Linear(d_model, d_model, bias=bias)\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Initiialize scaler\n",
    "        self.scaler = float(1.0 / math.sqrt(self.d_head)) # Store as float in initialization\n",
    "        \n",
    "\n",
    "    def forward(self, sequence, att_mask=None):\n",
    "        batch_size, seq_len, embed_dim = sequence.size()\n",
    "        \n",
    "        # Linear projections and reshape for multi-head\n",
    "        Q_state = self.q_proj(sequence) #[batch_size, seq_len, d_model=num_head * d_head]\n",
    "        K_state = self.k_proj(sequence)\n",
    "        V_state = self.v_proj(sequence)\n",
    "        \n",
    "        Q_state = Q_state.view(batch_size, seq_len, self.num_head, self.d_head).transpose(1,2) #[batch_size, self.num_head, seq_len, self.d_head]\n",
    "        K_state = K_state.view(batch_size, seq_len, self.num_head, self.d_head).transpose(1,2)\n",
    "        V_state = V_state.view(batch_size, seq_len, self.num_head, self.d_head).transpose(1,2)\n",
    "    \n",
    "       \n",
    "        # Scale Q by 1/sqrt(d_k)\n",
    "        Q_state = Q_state * self.scaler\n",
    "    \n",
    "    \n",
    "        # Compute attention matrix: QK^T\n",
    "        att_matrix = torch.matmul(Q_state, K_state.transpose(-1,-2)) \n",
    "\n",
    "    \n",
    "        # apply attention mask to attention matrix\n",
    "        if att_mask is not None and not isinstance(att_mask, torch.Tensor):\n",
    "            raise TypeError(\"att_mask must be a torch.Tensor\")\n",
    "\n",
    "        if att_mask is not None:\n",
    "            \n",
    "            # Expand mask for multi-head attention\n",
    "            # [batch_size, seq_len] -> [batch_size, 1, 1, seq_len]\n",
    "            att_mask = att_mask.unsqueeze(1).unsqueeze(2)\n",
    "            att_matrix = att_matrix.masked_fill(att_mask == 0, float('-inf'))\n",
    "        \n",
    "        # apply softmax to the last dimension to get the attention score: softmax(QK^T)\n",
    "        att_score = F.softmax(att_matrix, dim = -1)\n",
    "    \n",
    "        # apply drop out to attention score\n",
    "        att_score = self.dropout(att_score)\n",
    "    \n",
    "        # get final output: softmax(QK^T)V\n",
    "        att_output = torch.matmul(att_score, V_state)\n",
    "    \n",
    "        # concatinate all attention heads\n",
    "        att_output = att_output.contiguous().view(batch_size, seq_len, self.num_head*self.d_head) \n",
    "    \n",
    "        # final linear transformation to the concatenated output\n",
    "        att_output = self.output_proj(att_output)\n",
    "\n",
    "        return att_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bedf8d01-61e9-472b-836d-eb79d228da5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class FFN(nn.Module):\n",
    "    \"\"\"\n",
    "    Position-wise Feed-Forward Networks\n",
    "    This consists of two linear transformations with a ReLU activation in between.\n",
    "    \n",
    "    FFN(x) = max(0, xW1 + b1 )W2 + b2\n",
    "    d_model: embedding dimension (e.g., 512)\n",
    "    d_ff: feed-forward dimension (e.g., 2048)\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.d_model=d_model\n",
    "        self.d_ff= d_ff\n",
    "        \n",
    "        # Linear transformation y = xW+b\n",
    "        self.fc1 = nn.Linear(self.d_model, self.d_ff, bias = True)\n",
    "        self.fc2 = nn.Linear(self.d_ff, self.d_model, bias = True)\n",
    "        \n",
    "        # for potential speed up\n",
    "        # Pre-normalize the weights (can help with training stability)\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        # check input and first FF layer dimension matching\n",
    "        batch_size, seq_length, d_input = input.size()\n",
    "        assert self.d_model == d_input, \"d_model must be the same dimension as the input\"\n",
    "\n",
    "        # First linear transformation followed by ReLU\n",
    "        # There's no need for explicit torch.max() as F.relu() already implements max(0,x)\n",
    "        f1 = F.relu(self.fc1(input))\n",
    "\n",
    "        # max(0, xW_1 + b_1)W_2 + b_2 \n",
    "        f2 =  self.fc2(f1)\n",
    "\n",
    "        return f2\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a555a9d-29a7-46a6-b163-06934db1aacf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFN(\n",
      "  (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = FFN(  d_model = 512,  d_ff =2048)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89c2eebd-d774-4734-b9ea-486183182d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder layer of the Transformer\n",
    "    Sublayers: MultiHeadSlefAttention\n",
    "              FNN\n",
    "    Args:\n",
    "            d_model: 512 model hidden dimension\n",
    "            d_embed: 512 embedding dimension, same as d_model in transformer framework\n",
    "            d_ff: 2048 hidden dimension of the feed forward network\n",
    "            num_head: 8 Number of attention heads.\n",
    "            dropout:  0.1 dropout rate \n",
    "            \n",
    "            bias: Whether to include bias in linear projections.\n",
    "              \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, d_model, d_embed, d_ff,\n",
    "        num_head, dropout=0.1,\n",
    "        bias=True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_embed = d_embed\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "\n",
    "        # attention sublayer\n",
    "        self.att = MultiHeadSelfAttention(\n",
    "            d_model = d_model,\n",
    "            d_embed = d_embed,\n",
    "            num_head = num_head,\n",
    "            dropout = dropout,\n",
    "            bias = bias\n",
    "        )\n",
    "        \n",
    "        # FFN sublayer\n",
    "        self.ffn = FFN(\n",
    "            d_model = d_model,\n",
    "            d_ff = d_ff\n",
    "        )\n",
    "\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # layer-normalization layer\n",
    "        self.LayerNorm_att = nn.LayerNorm(self.d_model)\n",
    "        self.LayerNorm_ffn = nn.LayerNorm(self.d_model)\n",
    "\n",
    "\n",
    "    def forward(self, embed_input, att_mask):\n",
    "        \n",
    "        ## First sublayer: self attion \n",
    "        # After embedding and positional encoding, input sequence feed into attention sublayer\n",
    "        att_sublayer = self.att(sequence = embed_input, att_mask = att_mask)  # [batch_size, sequence_length, d_model]\n",
    "        # apply dropout before layer normalization for each sublayer\n",
    "        att_sublayer = self.dropout(att_sublayer)\n",
    "        # Residual layer normalization\n",
    "        att_normalized = self.LayerNorm_att(embed_input + att_sublayer)         # [batch_size, sequence_length, d_model]\n",
    "        \n",
    "        # Second sublayer: FFN\n",
    "        ffn_sublayer = self.ffn(att_normalized)                                 # [batch_size, sequence_length, d_model]\n",
    "        ffn_sublayer = self.dropout(ffn_sublayer)\n",
    "        ffn_normalized = self.LayerNorm_att(att_sublayer + ffn_sublayer )       # [batch_size, sequence_length, d_model]\n",
    "    \n",
    "\n",
    "        return ffn_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bed1d90-9b3b-468d-a746-d1d0e1753c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerEncoder(\n",
      "  (att): MultiHeadSelfAttention(\n",
      "    (q_proj): Linear(in_features=258, out_features=512, bias=True)\n",
      "    (k_proj): Linear(in_features=258, out_features=512, bias=True)\n",
      "    (v_proj): Linear(in_features=258, out_features=512, bias=True)\n",
      "    (output_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (ffn): FFN(\n",
      "    (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "    (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (LayerNorm_att): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (LayerNorm_ffn): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = TransformerEncoder( d_model = 512,  d_embed = 258, d_ff =2048, num_head=8, dropout=0.1, bias=True )\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cae577d2-6ad1-4e9f-ad9b-26c646ec7c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output Statistics:\n",
      "Mean: 0.0000\n",
      "Std: 1.0000\n",
      "Min: -4.0672\n",
      "Max: 3.6722\n",
      "\n",
      "Attention Analysis:\n",
      "Unmasked positions mean: 0.8028\n",
      "Masked positions mean: 0.8009\n",
      "\n",
      "Is the masking working? Yes\n",
      "\n",
      "All tests passed successfully!\n"
     ]
    }
   ],
   "source": [
    "def test_transformer_encoder():\n",
    "    # Set random seed for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    # Test parameters\n",
    "    batch_size = 32\n",
    "    seq_length = 20\n",
    "    d_embed = 512\n",
    "    d_model = 512\n",
    "    d_ff = 2048\n",
    "    num_heads = 8\n",
    "    \n",
    "    # Initialize the transformer encoder\n",
    "    encoder = TransformerEncoder(\n",
    "        d_model=d_model,\n",
    "        d_embed=d_embed,\n",
    "        d_ff=d_ff,\n",
    "        num_head=num_heads,\n",
    "        dropout=0.1\n",
    "    )\n",
    "    \n",
    "    # Set to evaluation mode to disable dropout\n",
    "    encoder.eval()\n",
    "    \n",
    "    # Create input sequence - using ones instead of random values\n",
    "    # for easier interpretation of attention patterns\n",
    "    input_sequence = torch.ones(batch_size, seq_length, d_embed)\n",
    "    \n",
    "    # Create attention mask\n",
    "    attention_mask = torch.ones(batch_size, seq_length)\n",
    "    attention_mask[:, 15:] = 0  # Mask last 5 positions\n",
    "    \n",
    "    # Store attention patterns\n",
    "    attention_patterns = []\n",
    "    \n",
    "    # Define hook to capture attention scores\n",
    "    def attention_hook(module, input, output):\n",
    "        # We want to capture the attention scores before they're processed further\n",
    "        # This assumes your attention module returns the attention scores\n",
    "        attention_patterns.append(output)\n",
    "    \n",
    "    # Register the hook on the attention computation\n",
    "    encoder.att.register_forward_hook(attention_hook)\n",
    "    \n",
    "    # Perform forward pass\n",
    "    with torch.no_grad():\n",
    "        output = encoder(input_sequence, attention_mask)\n",
    "    \n",
    "    # Basic shape tests\n",
    "    expected_shape = (batch_size, seq_length, d_model)\n",
    "    assert output.shape == expected_shape, f\"Expected shape {expected_shape}, got {output.shape}\"\n",
    "    \n",
    "    # Print output statistics\n",
    "    print(\"\\nOutput Statistics:\")\n",
    "    print(f\"Mean: {output.mean():.4f}\")\n",
    "    print(f\"Std: {output.std():.4f}\")\n",
    "    print(f\"Min: {output.min():.4f}\")\n",
    "    print(f\"Max: {output.max():.4f}\")\n",
    "    \n",
    "    # Analyze attention patterns\n",
    "    if attention_patterns:\n",
    "        attention_output = attention_patterns[0]\n",
    "        # Look at the attention patterns for unmasked vs masked positions\n",
    "        unmasked_attention = output[:, :15, :].abs().mean()\n",
    "        masked_attention = output[:, 15:, :].abs().mean()\n",
    "        \n",
    "        print(\"\\nAttention Analysis:\")\n",
    "        print(f\"Unmasked positions mean: {unmasked_attention:.4f}\")\n",
    "        print(f\"Masked positions mean: {masked_attention:.4f}\")\n",
    "        \n",
    "        # Note: We expect masked positions to still have values due to residual connections,\n",
    "        # but their patterns should be different from unmasked positions\n",
    "        print(\"\\nIs the masking working?\", \"Yes\" if unmasked_attention != masked_attention else \"No\")\n",
    "    \n",
    "    # Check for any NaN or infinite values\n",
    "    assert torch.isfinite(output).all(), \"Output contains NaN or infinite values\"\n",
    "    \n",
    "    print(\"\\nAll tests passed successfully!\")\n",
    "    return output, attention_patterns\n",
    "\n",
    "# Run the test\n",
    "output, attention_patterns = test_transformer_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3aca0ac-3b48-4033-a889-dd019e2d67a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
